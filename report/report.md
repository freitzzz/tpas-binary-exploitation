# Binary Exploitation of Figlet Unix Binary

The present report, written in the context of TPAS (Teoria e Prática de Ataques de Segurança) curricular unit at FCUP (Faculdade de Ciências da Universidade do Porto), aims at describing the process of attempting to find bugs and crashes in a command-line binary for Unix-like systems.
It will start by describing the goals of the project, methodologies being followed, approaches to take and technologies to look on. Then, it will be presented the binary in context of analysis, depicting initial weakness and the reasons to why it was chosen as the target binary. Following this, the working environment and tools used for exploitation will be described in detail, focusing on the quirks and constraints felt during the setup phase. The fourth chapter describes the fuzzing process and achieved results in an iterative manner. 
To conclude, an overview of the overall effectiveness and complexity of performing such analysis will be presented, as well as some remarks for future work.

## 1. Terminologies, Methodologies, Technologies and Goals

Binary exploitation can be defined as the process of "finding a vulnerability in the program and exploiting it to gain control of a shell or modifying the program's functions" [1].
To gain control of a shell, typically newbie attackers inject shellcode in the program through an input stream (e.g., user input(stdin)), which allows for arbitrary code execution (i.e., using a program as the interpreter of crafted binary code). Shellcode is a special type of code produced with assembly instructions, for a specific CPU architecture [2].

Despite shellcode attacks still being popular and effective for a wide range of use cases, more advanced attackers rely on *Return Oriented Programming* (ROP), a technique that exploits the program call stack by hijacking the memory and aligning the assembly instructions present on it to execute code in an arbitrary way [3].
This technique is way more effective on modern age binaries, as these are protected with specific security flags on compile time (e.g., NX, ASLR, Stack Canary) that detect if whether a program was compromised or not during runtime, via a memory corruption [3].

**But how do attackers find vulnerabilities that allow arbitrary code execution?**

To craft code and execute it in a target program, we first need to load it in the program memory so it can be arbitrarily triggered and executed by the program. Since modern operating systems reserve specific memory spaces for a program, no entity can write to this memory in an external manner. So, to write in the program memory, it is needed to *conduct the program to do it so*. This is where the vulnerabilities take place: there is a need to find usage of unsafe operations in a binary, so that specific input or program flow can corrupt the reserved memory. Classic vulnerabilities include:

- **Buffer Overflow**, where data bigger than the actual buffer (i.e., memory space) is written to it, which causes this data to be propagated in the memory, until a crash occurs [4];
- **Use After Free (UAF)**, which occurs when a specific chunk of memory is used after it was deallocated/freed [4, 5];
- **Format Strings**, which occurs when programs that use operations that rely on format strings (e.g., `printf` and `sprintf`) are vulnerable to format operations injection, which will force format strings functions to look for arguments to replace on the format position [4, 5].

```
char buf[8];

void vulnerable() {
    gets(buf);
}
```

Code Snippet 1 - Example of a buffer overflow. The `gets` function does not check for the buffer length, so any input with length greater than **8**, will cause a buffer overflow. Source: [5].

One very important thing to note, is that memory corruptions are far more easy to occur in "low level" programming languages such as `C` and `C++`, rather in high level ones (e.g., `Javascript, C#`), as the former allows for direct dynamic memory allocation and does not have memory optimizers, such as *garbage collection*. This gives an hint on the programs to choose for trying to find memory vulnerabilities.

**Okay, this is great and seems easy. Can I hack the government with binary exploitation?**

While theoretically you sure can exploit any binary to arbitrarily execute code, these kind of attacks have been disclosed since the late 80s, and so there is already a lot progress on detecting memory vulnerabilities in software, both during compile and runtime. So most likely you cannot exploit any binary by just slapping fingers, but that does not mean that exploiting the binary is impossible!

The state-of-the-art for analyzing and exploiting binaries divides these processes in two approaches: the **white-box** approach, where one has access to the source-code of the binary and as such can find vulnerabilities in a static analysis manner (i.e., read code and find bugs); and the **black-box** approach, which assumes that the binary is a black-box that no one knows how it is built, giving input to it and expecting to receive a specific output.
One can argue that white-box analysis is the best approach to follow, as an experienced security researcher or programmer knows every weakness of a programming language. However, access to the source-code is not always available. Usually, the community relies on the black-box approach through an automated process called ***Fuzzing***.

## 1.1 What is Fuzzing?

OWASP defines fuzzing as a "black box software testing technique, which basically consists in finding implementation bugs using malformed/semi-malformed data injection in an automated fashion" [6]. It is very similar to property-based testing (PBT), where by producing random input and applying mutations to customize it, the program is faced with inputs it may have not been considering, leading to unexpected or undesired behaviour.

Fuzzing is an automatic process, which makes finding bugs an exciting process! Security researchers can just leave their machines *fuzzing* the binary during the night and analyze the collected results during the day. By default, it is efficient. However, it does not grant effectiveness, as it is still the job of the researcher to prepare the fuzzing *campaign* and guide it.

![fuzzing_overview](https://raw.githubusercontent.com/freitzzz/tpas-binary-exploitation/master/report/figures/fuzzing_overview.png)

Figure 1 - Fuzzing simplified in a state diagram. You can't get a better illustrative description than this! Source: [7].

## 1.2 What is the trend for fuzzing technologies today?

There are different open-source tools available for fuzzing binaries, each with their own quirks and advantages. The Google fuzzing team highlights the following as the most performant ones [8], based on the benchmarks provided by the `FuzzBench` [9] tool:

- `libFuzzer`, an imperative fuzzer engine which interprets fuzzing tests written in `C` and runs them [8, 10]. libFuzzer focuses on maximizing code coverage [10], which means that it is only useful if the researcher has knowledge about the underlying source-code;
- `Honggfuzz`, a fuzzer recognized for its performance and customization options [8], serving as an alternative where defining fuzzing tests is not possible [11]. Not only it supports software, but also hardware fuzzing;
- `American Fuzzy Lop (afl)`, a guided fuzzing engine that is fast and widely recognized in the fuzzing community [8]. AFL major advantage is the fact that it is guided (i.e., fuzzer learns and reinforces the input based on crash results) [12].

The authors of the report understand that some of these fancy fuzzing terminologies are very confusing, so a glossary defined by the Google Fuzzing team [12] is provided in Appendix A.

## 1.3 What are the Project Goals?

The overall goal and purpose of the present project, was to get introduced in binary exploitation and try to get a crash in a target binary/program. To complement this goal, the report authors also desired to get a **CVE** (Common Vulnerability Exposure) for the found crash, meaning that the crash would require to have a pratical **PoV** (Proof of Vulnerability) to back the legitimacy and impact of the crash.

As for the report goals, the authors would like to provide a clear and descriptive framework for binary exploitation, so that newbie security researchers can understand the processes to take with easy, as well as to get their hands dirty in the shortest time.

## 2. Choosing the binary to exploit

Choosing a target binary to exploit, is as difficult as choosing a nickname in an online video-game: there are so many possibilities - ones more difficult than the others -. which provide a higher satisfaction of working on and getting results. It was suggested by the professor assisting the project, that the binary being picked falls under the following categories:

- Linux tools, as the majority of these are open-source and widely used;
- Video-games, because although the increase of complexity, it would be very rewarding to get a crash and at the same time be rewarded in a bug-bounty program.

From the beggining we know that we want to get a crash in order to exploit and possibly get a CVE, which is far more easy on Linux tools. But looking to video-games, getting a crash is way more rewarding than a rather simple Linux tool, altough that these involve way more complex codebases, meaning that it is far more difficult to find a crash.

Since this project is being developed in the context of the Information Security Masters @ FCUP, TPAS course, there are four more courses which the report authors are enrolled in, meaning that the available time and resources to provide on the project is scarce. Having this constraint in mind, and also taking in consideration that both authors are "working-students", we decided to stick with the easiest path, navigating towards the Linux tools path.

**There are thousands of Linux tools, which shall be picked?**

When searching for target Linux binaries, the authors had some extra constraints in consideration: we want to publish a write-up of our findings (the present report) publicly and tackle some software that is used everyday by Linux users. This means that the binary must be open-source (to avoid law violations) and have some reputation in the Linux or open-source community. Additionally, the binary should also be written in `C` or `C++` to ensure that memory safety can be exploited.

We have then embarked in a journey of finding a tool that best fits our requirements. Of all the tools reviewed, the following three caught our attention the most:

- `jq`, a command-line JSON processor, widely used by developers, taking as input [14];
- `sudo`, the classic tool to leverage user permissions to root, shipped in almost every Linux distribution and macOS [15];
- `figlet`, a tool that takes input from stdin and outputs a "formatted art message", according to a specific font [16].

The `jq` and `figlet` tools are very similar in terms on how these behave: they take some user input, process it, and spit some output. `sudo` is particularly interesting for the use case of gaining access to a root shell, as its goal is to run programs with root permissions.

There are many benefits in choosing each of the presented tools, however we can only choose one, so it must be a wise choice. Finding a crash in the `sudo` binary would be a pretty neat challenge, not only because it is the most widely used tool of the presented ones, but it is also a reference for fuzzing by security researchers [17]. The latest crash in `sudo` (or more precisely `sudoedit`) was found in 2021, having the **CVE-2021-3156** assigned to it [17, 18].

It is pretty safe to say that finding a crash in sudo will be hard, so let's focus on the remaining tools. The decision between whether picking `jq` or `figlet`, comes down to the hardness constraint. Upon the time of writing this report, it seems only a few people have tried and published the results for fuzzing figlet, while jq already has some publised write-ups [19, 20] (note: we used the keywords "fuzzing figlet" and "fuzzing jq" as the search query in Google, to find these results).
We know that we should not judge a book by its cover (in this context, it relates to the hardness of finding a bug if someone has fuzzed or found one yet), but it seemed that figlet is the best tool to pick, as we would be sort of pioneers in its fuzzing. In addition, the figlet codebase can be summarized in one file of `~1.8k SLOC` (Single Lines of Code), meaning that it is not a complex binary, while jq codebase is far more bigger.

## 3. Choosing the research approach and tools

It is known that `figlet` will be the binary of study, but before getting our hands dirty, we need to establish our working model. Shall we follow the white-box or black-box approach? Figlet is open-source and so white-box is possible, but automating the crash finding seems way more fun and interesting.

**If we follow the black-box approach, which tool should we adopt?**

Previously in Chapter 1, we have seen the following fuzzing tools: `libFuzz`, `HongFuzz` and `AFL`. At first glance, using `libFuzzer` seems a good idea, as we have an extensive support for different kind of configurations via a test definition. However, we cannot forget that this fuzzer focuses on checking code coverage, like in unit tests, and so it might impact the research taking place. The fuzzers `AFL` and `HongFuzz` are both very performant, but the guided property of AFL is what makes it special. If we believe that the binary will behave differently on a certain input, we can sure guide AFL to fuzz it and learn from the results. It seems clear that choosing AFL will provide the best results, but that does not mean that HongFuzz is not good, as both have their quirks.

![trump_rambo](https://raw.githubusercontent.com/freitzzz/tpas-binary-exploitation/master/report/figures/trump_rambo.png)

Figure 2 - AFL stands for "American Fuzzy Lop", or as we like to call it "American Fuzileiros", because you are essentially constantly firing rounds of input to the binary (This picture is obviously satire). Source: [21].

Although starting on with a black-box approach, we have later adopted a hybrid approach, by also performing static analysis of Figlet source-code. The reasons for this strategy will be explained later in Chapter 5.

## 4. Fuzzing setup

Explicar por tinttim setup com AFL++
Explicar problemas que se sentiram no setup
Providenciar um "Hello World" para garantir que setup funciona

## 6. Final Remarks and Future Improvements

- Test different fuzzing engines;
- Test different instrumenting targets;

### References

[1] CTF101, ‘Overview - CTF 101: Binary Exploitation’, CTF101, 2019 https://ctf101.org/binary-exploitation/overview/(accessed Feb. 8, 2022).

[2] Firewalls.com, ‘Shellcode - Firewalls.com’, Firewalls.com, 2022 https://www.firewalls.com/blog/security-terms/shellcode/#:~:text=Shellcode%20is%20a%20special%20type,control%20of%20the%20affected%20system.(accessed Feb. 8, 2022).

[3] Jonathan Salwan, ‘An introduction to the Return Oriented Programming and ROP chain generation’, 2014 http://shell-storm.org/talks/ROP_course_lecture_jonathan_salwan_2014.pdf(accessed Feb. 8, 2022).

[4] CTF101, ‘Heap Exploitation - CTF 101’, CTF101, 2019 https://ctf101.org/binary-exploitation/heap-exploitation/(accessed Feb. 8, 2022).

[5] CS161, ‘Memory Safety Vulnerabilities | Computer Security’, textbook.cs161.org, 2022 https://textbook.cs161.org/memory-safety/vulnerabilities.html(accessed Feb. 8, 2022).

[6] OWASP, ‘Fuzzing | OWASP Foundation’, owasp.org, 2022 https://owasp.org/www-community/Fuzzing(accessed Feb. 8, 2022).

[7] Xavi, ‘Fuzzing – Finding bugs using BooFuzz (1/3) | Happy Hacking!’, xavibel.com, 2019 https://xavibel.com/2019/06/23/fuzzing-how-to-find-bugs-using-boofuzz/(accessed Feb. 8, 2022).

[8] google/fuzzing, ‘fuzzing/intro-to-fuzzing.md at master · google/fuzzing’, github.com, 2020 https://github.com/google/fuzzing/blob/master/docs/intro-to-fuzzing.md(accessed Feb. 9, 2022).

[9] FuzzBench, ‘FuzzBench’, fuzzbench.com, 2022 https://www.fuzzbench.com/(accessed Feb. 9, 2022).

[10] llvm, ‘libFuzzer – a library for coverage-guided fuzz testing. — LLVM 15.0.0git documentation’, llvm.org, 2022 https://llvm.org/docs/LibFuzzer.html(accessed Feb. 9, 2022)

[11] Honggfuzz, ‘google/honggfuzz: Security oriented software fuzzer. Supports evolutionary, feedback-driven fuzzing based on code coverage (SW and HW based)’, github.com, 2022 https://github.com/google/honggfuzz(accessed Feb. 9, 2022)

[12] google/AFL, ‘google/AFL: american fuzzy lop - a security-oriented fuzzer’, github.com, 2022 https://github.com/google/AFL(accessed Feb. 9, 2022)

[13] google/fuzzing, ‘fuzzing/glossary.md at master · google/fuzzing’, github.com, 2020 https://github.com/google/fuzzing/blob/master/docs/glossary.md(accessed Feb. 9, 2022)

[14] jq, ‘jq is a lightweight and flexible command-line JSON processor.’, stedolan.github.io, 2018 https://stedolan.github.io/jq/(accessed Feb. 9, 2022)

[15] sudo, ‘sudo(8): execute command as another user - Linux man page’, linux.die.net, 2012 https://linux.die.net/man/8/sudo(accessed Feb. 9, 2022)

[16] Figlet, ‘cmatsuoka/figlet: Claudio's FIGlet tree’, github.com, 2020 https://github.com/cmatsuoka/figlet(accessed Feb. 9, 2022)

[17] LiveOverflow, ‘How SUDO on Linux was HACKED! // CVE-2021-3156’, youtube.com, 2021 https://www.youtube.com/watch?v=TLa2VqcGGEQ(accessed Feb. 10, 2022)

[18] CVE/MITRE, ‘CVE - CVE-2021-3156’, cve.mitre.org, 2021 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-3156(accessed Feb. 10, 2022)

[19] David Maclver, ‘Use after free bug · Issue #896 · stedolan/jq’, github.com, 2015 https://github.com/stedolan/jq/issues/896(accessed Feb. 10, 2022)

[20] Harrison Green, ‘Learning About Structure-Aware Fuzzing and Finding JSON Bugs to Boot’, forallsecure.com, 2020 https://forallsecure.com/blog/learning-about-structure-aware-fuzzing-and-finding-json-bugs-to-boot(accessed Feb. 10, 2022)

[21] Redneck Nation, ‘Trump Rambo Sticker - Redneck Nation’, rednecknationstrong.com, 2022 https://rednecknationstrong.com/trump-rambo-sticker/(accessed Feb. 10, 2022)

[<ref_number>] <author>, ‘<title>’, <website_name>, <published_date>. <website_url>(accessed <month>. <day>, <year>).

### Appendixes

#### A – Glossary of Fuzzing Terminologies (Source: [13])

# Glossary

Naming things is hard, so this page tries to reduce confusion around fuzzing-related terminology.

## Corpus
Or **test corpus**, or **fuzzing corpus**.<BR>
A set of [test inputs](#test-input). In most contexts, it refers to a set of minimal test inputs that generate maximal code coverage.

## Cross-pollination
The term is taken from botany, where one plant pollinates a plant of another variety.
In fuzzing, cross-pollination means using a corpus for one
[fuzz target](#fuzz-target) to expand a [corpus](#corpus) for another fuzz target.
For example, if there are two libraries that process the same common data
format, it is often benefitial to cross-pollinate their respective corpora.

## Dictionary
A file which specifies interesting tokens for a [fuzz target](#fuzz-target).
Most [fuzzing engines](#fuzzing-engine) support dictionaries, and will adjust
their mutation strategies to process these tokens together.

## Fuzz Target
Or **Target Function**, or **Fuzzing Target Function**, or **Fuzzing Entry Point**.<BR>
A function to which we apply fuzzing. A [specific signature](http://libfuzzer.info#fuzz-target) is required for OSS-Fuzz.
Examples: [openssl](https://github.com/openssl/openssl/blob/master/fuzz/x509.c),
[re2](https://github.com/google/re2/blob/master/re2/fuzzing/re2_fuzzer.cc),
[SQLite](https://www.sqlite.org/src/artifact/ad79e867fb504338).

## Fuzzer
The most overloaded term and used in a variety of contexts, which makes it bad.
Sometimes, "Fuzzer" is referred to a [fuzz target](#fuzz-target),
a [fuzzing engine](#fuzzing-engine),
a [mutation engine](#mutation-engine),
a [test generator](#test-generator) or
a [fuzzer build](#fuzzer-build).

## Fuzzer Build
A build that contains all the fuzz targets for a given project, which is run with a specific fuzzing engine, in a specific build mode (e.g. with enabled/disabled assertions), and optionally combined with a sanitizer.
In [OSS-Fuzz](https://google.github.io/oss-fuzz/), it is also known as a [job type](https://google.github.io/oss-fuzz/reference/glossary/#job-type).

## Fuzzing Engine
A tool that tries to find interesting inputs for a [fuzz target](#fuzz-target) by executing it.
Examples: [libFuzzer](http://libfuzzer.info),
[AFL](lcamtuf.coredump.cx/afl/),
[honggfuzz](https://github.com/google/honggfuzz), etc.

See related terms [Mutation Engine](#mutation-engine) and [Test Generator](#test-generator).

## Mutation Engine
A tool that takes a set of testcases as input and creates their mutated versions.
It is just a generator and does not feed the mutations to [fuzz target](#fuzz-target).
Example: [radamsa](https://github.com/aoh/radamsa) (a generic test mutator).

## Reproducer
Or **Test Case**.<BR>
A [test input](#test-input) that can be used to reproduce a bug when processed
by a fuzz target.

## [Sanitizer](https://github.com/google/sanitizers)
A [dynamic testing](https://en.wikipedia.org/wiki/Dynamic_testing) tool that can detect bugs during program execution.
Examples:
[ASan](http://clang.llvm.org/docs/AddressSanitizer.html),
[DFSan](http://clang.llvm.org/docs/DataFlowSanitizer.html),
[LSan](http://clang.llvm.org/docs/LeakSanitizer.html),
[MSan](http://clang.llvm.org/docs/MemorySanitizer.html),
[TSan](http://clang.llvm.org/docs/ThreadSanitizer.html),
[UBSan](http://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html).

## Seed Corpus
A small initial [corpus](#corpus) prepared with the intent of providing initial
coverage for fuzzing. Rather than being created by the fuzzers themselves, seed
corpora are often prepared from existing test inputs or may be hand-crafted to
provide interesting coverage. They are often checked into source alongside [fuzz
targets](#fuzz-target).

## Test Generator
A tool that generates testcases from scratch according to some rules or grammar.
Examples:
[csmith](https://embed.cs.utah.edu/csmith/) (a test generator for C language),
[cross_fuzz](http://lcamtuf.coredump.cx/cross_fuzz/) (a cross-document DOM binding test generator).

## Test Input
A sequence of bytes that is used as input to a [fuzz target](#fuzz-target).
Typically, a test input is stored in a separate file.